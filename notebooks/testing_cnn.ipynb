{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomy-lorant\u001b[0m (\u001b[33mlorant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from scipy import stats\n",
    "import esm\n",
    "\n",
    "\n",
    "# log into to wandb to log results\n",
    "import wandb\n",
    "\n",
    "torch.set_num_threads(24)\n",
    "\n",
    "WANDB_NOTEBOOK_NAME = '2024-06-13_mbe-linear.ipynb'\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "datadir = '../out/corrected/counts'\n",
    "procdir = '../out/modelling/processed'\n",
    "# out/corrected/counts/2024-06-05_r0_np-cc_gel-extract_aa-seq-counts.tsv.gz\n",
    "BATCH_SIZE=64\n",
    "\n",
    "\n",
    "os.makedirs(procdir, exist_ok=True)\n",
    "np.random.seed(12345)\n",
    "rerun_encoding = False\n",
    "\n",
    "\n",
    "# get files to be imported\n",
    "files = glob.glob(os.path.join(datadir, '2024-06-05_r[0-1]_np-cc*aa-seq-counts.tsv.gz'))\n",
    "\n",
    "# read tsv files\n",
    "df = pd.concat([pd.read_csv(f, sep='\\t').assign(round = os.path.basename(f)[11:13]) for f in files], axis=0)\n",
    "\n",
    "# sum counts within each round\n",
    "df = df.groupby(['round', 'sequence']).sum().reset_index()\n",
    "\n",
    "\n",
    "pseduo = 1\n",
    "df_unique = (df\n",
    "             # add pseudocunt\n",
    "             .assign(count = lambda x: x['count'] + pseduo)\n",
    "             # r0 and 1 counts in separate columns\n",
    "             .pivot(index='sequence', columns='round', values='count')\n",
    "             .fillna(pseduo)\n",
    "             .reset_index()\n",
    "             # calculate log enrichment and weights as per MBE paper\n",
    "             .assign(\n",
    "                le = lambda x: np.log2((x['r1']/x['r1'].sum())/(x['r0']/x['r0'].sum())),\n",
    "                sig = lambda x: 1/x['r1']*(1-x['r1']/(x['r1'].sum())) + 1/x['r0']*(1-x['r0']/(x['r0'].sum())),\n",
    "                weight = lambda x: 1/(2*x['sig']),\n",
    "                     )\n",
    "             )\n",
    "\n",
    "thresh = 1\n",
    "df_unique = df_unique.assign(set = lambda x: ['high' if w >= thresh else 'low' for w in x['weight']])\n",
    "\n",
    "df_unique = df_unique.loc[df_unique['set'] == 'high']\n",
    "\n",
    "df_test_eval = df_unique.sample(frac=1).head(100)\n",
    "df_test = df_test_eval.head(50)\n",
    "\n",
    "# remove test set from evaluation set\n",
    "df_eval = df_test_eval[~df_test_eval['sequence'].isin(df_test['sequence'])]\n",
    "df_train = df_unique[~df_unique['sequence'].isin(df_test_eval['sequence'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one hot encoding\n",
    "aas = [i for i in 'ACDEFGHIKLMNPQRSTVWY*']\n",
    "\n",
    "def esm_embedding(seqs, max_len):\n",
    "    # print(seqs)\n",
    "    \n",
    "    # data is a list of tuples (index, sequence)\n",
    "    data = [(str(round_val), sequence + \"<pad>\" * (max_len - len(sequence))) for round_val, sequence in seqs]\n",
    "    \n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter(max_len)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 2 ** 5 # = 32\n",
    "    prev = 0\n",
    "    next = batch_size if batch_size < len(data) else len(data)\n",
    "\n",
    "    labels = []\n",
    "    sequence_representations = []\n",
    "    while next <= len(data) and next != prev:\n",
    "        # print(\"Prev: \", prev, \" and Next: \", next)\n",
    "        batch_labels, batch_seq, batch_tokens = batch_converter(data[prev:next])\n",
    "        labels = labels + batch_labels\n",
    "        # print(\"Finished the batch items\")\n",
    "        # batch_lens is just an array with the length of all of the sequences\n",
    "        # and alphabet.padding_idx is jus thte index of the token '<pad>' in the alpahabet token list\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        # print(\"Finished no grad\")\n",
    "        # Generate per-sequence representations via averaging\n",
    "        # sequence_representations = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            sequence_representations.append((token_representations[i, 1 : tokens_len - 1]).numpy())\n",
    "        # print('Finished appending sequence representations')\n",
    "        prev = next\n",
    "        if next + batch_size > len(data):\n",
    "            next = len(data)\n",
    "        else:\n",
    "            next += batch_size\n",
    "\n",
    "        del batch_lens\n",
    "        del batch_tokens\n",
    "        del batch_seq\n",
    "        del batch_labels\n",
    "        del results\n",
    "        del token_representations\n",
    "\n",
    "    \n",
    "    return labels, sequence_representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Encdoding the dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode(df, max_len):\n",
    "    # Extract the index (or this case the round number) and the amino acid sequence of each entry in the data frame\n",
    "    # index_and_sequence = [(round_val, df.loc[round_val, 'sequence'][:len(df.loc[round_val, 'sequence']) - 1]) for round_val in df.index.tolist()]\n",
    "    # index_and_sequence = [(round_val, df.loc[round_val, 'sequence'].replace('*', '<eos>')) for round_val in df.index.tolist()]\n",
    "    index_and_sequence = [(round_val, ((df.loc[round_val, 'sequence'])[:-1] + '<unk>'*(max_len - len(df.loc[round_val, 'sequence'])) + '<eos>')) for round_val in df.index.tolist()]\n",
    "    indexes, encoded_numpy_arr= esm_embedding(index_and_sequence, max_len)\n",
    "    # print(\"Finished embedding\")\n",
    "    # numpy_arrays = [tensor.numpy() for tensor in encoded_tensor]\n",
    "    \n",
    "    df_encoded = pd.DataFrame({\n",
    "        'round': indexes,\n",
    "        'encoded': encoded_numpy_arr\n",
    "    })\n",
    "\n",
    "    df_encoded.set_index('round', inplace=True)\n",
    "    df_encoded.index = df_encoded.index.astype('int64')\n",
    "\n",
    "    out = df\n",
    "    # out['encoded'] = df_encoded\n",
    "    out.loc[:,'encoded'] = df_encoded\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the 2d arrays to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1320425/2494140741.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out.loc[:,'encoded'] = df_encoded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1320425/2494140741.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out.loc[:,'encoded'] = df_encoded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1320425/2494140741.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out.loc[:,'encoded'] = df_encoded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Test\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "('Can only convert 1-dimensional array values', 'Conversion failed for column encoded with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved Test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# save data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_ESM_embedding_2d.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m df_eval\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(procdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_ESM_embedding_2d.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     16\u001b[0m df_test\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(procdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_ESM_embedding_2d.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pandas/io/parquet.py:190\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 190\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    193\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/table.pxi:4525\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/pandas_compat.py:624\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m--> 624\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_fut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/pandas_compat.py:598\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    602\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/pandas_compat.py:592\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    589\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/array.pxi:345\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/array.pxi:85\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/aavolve_data/.micromamba/envs/torch2/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('Can only convert 1-dimensional array values', 'Conversion failed for column encoded with type object')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_seq_length = df_unique['sequence'].str.len().max() + 10\n",
    "# rerun_encoding = True\n",
    "\n",
    "if not os.path.exists(os.path.join(procdir, 'train_ESM_embedding_2d.parquet')) or rerun_encoding:\n",
    "    print(\"Saving to files\")\n",
    "    \n",
    "    df_train = encode(df_train, max_seq_length)\n",
    "    print('Saved Train')\n",
    "    df_eval = encode(df_eval, max_seq_length)\n",
    "    print(\"Saved Eval\")\n",
    "    df_test = encode(df_test, max_seq_length)\n",
    "    print(\"Saved Test\")\n",
    "    # save data\n",
    "    df_train.to_parquet(os.path.join(procdir, 'train_ESM_embedding_2d.parquet'))\n",
    "    df_eval.to_parquet(os.path.join(procdir, 'eval_ESM_embedding_2d.parquet'))\n",
    "    df_test.to_parquet(os.path.join(procdir, 'test_ESM_embedding_2d.parquet'))\n",
    "\n",
    "else:\n",
    "    df_train = pd.read_parquet(os.path.join(procdir, 'train_ESM_embedding_2d.parquet')) # too large\n",
    "    df_eval = pd.read_parquet(os.path.join(procdir, 'eval_ESM_embedding_2d.parquet'))\n",
    "    df_test = pd.read_parquet(os.path.join(procdir, 'test_ESM_embedding_2d.parquet'))\n",
    "    # max_seq_length would not be defined if the dfs were already loaded in an external file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MBEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df_raw = df\n",
    "        self.df = self.create_mbe_dataset(df)\n",
    "    \n",
    "    def create_mbe_dataset(self, df):\n",
    "\n",
    "        df = (df\n",
    "              .loc[:, ['encoded', 'r0', 'r1']]\n",
    "              .melt(id_vars=['encoded'], value_vars=['r0', 'r1'], var_name='round', value_name='value')\n",
    "              .assign(label = lambda x: (x['round'] == 'r1').astype(int))\n",
    "        )\n",
    "        df = df.reindex(df.index.repeat(df['value'])).reset_index()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Number of examples in each class\n",
    "        this is just used to adjust for the different number of coutns in each round (r0 and r1)\n",
    "        \"\"\"\n",
    "        counts = (self.df\n",
    "                .groupby('label')\n",
    "                .size()\n",
    "               )\n",
    "        return torch.tensor(counts.loc[1] / counts.loc[0], dtype=torch.float64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # change this into geting the data from the files rather than \n",
    "        row = self.df.iloc[idx]\n",
    "        return row['encoded'], float(row['label'])\n",
    "\n",
    "class LEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Log enrichment dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        return row['encoded'], row['le']\n",
    "\n",
    "class MBEDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, enc=\"ESM_embedding\", stage=None):\n",
    "        self.train = pd.read_parquet(os.path.join(self.data_dir, f'train_{enc}.parquet'))\n",
    "        self.eval = pd.read_parquet(os.path.join(self.data_dir, f'eval_{enc}.parquet'))\n",
    "        self.test = pd.read_parquet(os.path.join(self.data_dir, f'test_{enc}.parquet'))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MBEDataset(self.train), batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(LEDataset(self.eval), batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(LEDataset(self.test), batch_size=self.batch_size)\n",
    "    \n",
    "# test out dataset class\n",
    "ds = MBEDataset(df_eval)\n",
    "\n",
    "# test out datamodule class - takes a while to load pickled data\n",
    "dm = MBEDataModule(procdir, batch_size=BATCH_SIZE)\n",
    "dm.setup()\n",
    "# test = next(iter(dm.train_dataloader()))\n",
    "# test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what the output looks like so I can get the dimensions of the data to pass to the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train.head()[0].shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module): # This used to be called LSTM\n",
    "    # input is 750 x 1270 (check the number of columns)\n",
    "    def __init__(self, input_rows, input_cols, initial_channels, pos_weight):\n",
    "        super(self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This will always return the probabilites\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        p = self.forward(x)\n",
    "        return p/(1-p) / self.pos_weight\n",
    "\n",
    "model = CNN(len(ds[0][0]),3, 1, ds.get_weights())\n",
    "# model(test[0])[:5]\n",
    "\n",
    "class LitMBE(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr=1e-5, pos_weight = 1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.spearman = stats.spearmanr \n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.type(torch.float64)\n",
    "        y = y.type(torch.float64)\n",
    "        logits = self.model(x).squeeze()\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss, on_step = True, on_epoch = True, prog_bar = True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.type(torch.float64)\n",
    "        y = y.type(torch.float64)\n",
    "        with torch.no_grad():\n",
    "            le = self.model.predict(x).squeeze().cpu().numpy()\n",
    "            spearman = self.spearman(le, y.cpu().numpy()).statistic\n",
    "            self.log('val_spearman', spearman, on_step = False, on_epoch = True, prog_bar = True)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "# instantiate lightning model\n",
    "lit_model = LitMBE(model, pos_weight=ds.get_weights())\n",
    "\n",
    "# use weights and biases logger\n",
    "wandb_logger = WandbLogger(project='mbe', name = \"LSTM ESM2 no wd\")\n",
    "wandb_logger.experiment.config.update({\n",
    "    \"lr\": 0.001,\n",
    "    \"pos_weight\": ds.get_weights(),\n",
    "    \"n_units\": 128,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"arch\": \"LSTM\",\n",
    "    \"enc\": \"ESM2\",\n",
    "    \"loss\": \"MSELoss\",\n",
    "    \"opt\": \"Adam\",\n",
    "    \"weight_decay\": 0\n",
    "})\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(max_epochs=10, logger = wandb_logger)\n",
    "trainer.validate(model = lit_model, datamodule = dm)\n",
    "trainer.fit(model = lit_model, datamodule = dm)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
