{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomy-lorant\u001b[0m (\u001b[33mlorant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import esm\n",
    "import dask as dd\n",
    "\n",
    "# log into to wandb to log results\n",
    "import wandb\n",
    "\n",
    "WANDB_NOTEBOOK_NAME = '2024-06-13_mbe-linear.ipynb'\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "datadir = '../out/corrected/counts'\n",
    "procdir = '../out/modelling/processed'\n",
    "BATCH_SIZE=64\n",
    "\n",
    "os.makedirs(procdir, exist_ok=True)\n",
    "np.random.seed(12345)\n",
    "rerun_encoding = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "Load the r0 and r1 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../out/corrected/counts/2024-06-05_r0_np-cc_aa-seq-counts.tsv.gz',\n",
       " '../out/corrected/counts/2024-06-05_r1_np-cc_aa-seq-counts.tsv.gz',\n",
       " '../out/corrected/counts/2024-06-05_r0_np-cc_gel-extract_aa-seq-counts.tsv.gz',\n",
       " '../out/corrected/counts/2024-06-05_r0_np-cc_repeat_aa-seq-counts.tsv.gz',\n",
       " '../out/corrected/counts/2024-06-05_r1_np-cc_gel-extract_aa-seq-counts.tsv.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get files to be imported\n",
    "files = glob.glob(os.path.join(datadir, '2024-06-05_r[0-1]_np-cc*aa-seq-counts.tsv.gz'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>sequence</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r0</td>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r0</td>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r0</td>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r0</td>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r0</td>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNRRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243361</th>\n",
       "      <td>r1</td>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWALKPGVPQPKANQQHQDNRRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243362</th>\n",
       "      <td>r1</td>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWALKPGVPQPKANQQHQDNRRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243363</th>\n",
       "      <td>r1</td>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDGRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243364</th>\n",
       "      <td>r1</td>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHQDNSRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243365</th>\n",
       "      <td>r1</td>\n",
       "      <td>MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1243366 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round                                           sequence  count\n",
       "0          r0  MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...      1\n",
       "1          r0  MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...      1\n",
       "2          r0  MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...      1\n",
       "3          r0  MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...      1\n",
       "4          r0  MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNRRGLV...      1\n",
       "...       ...                                                ...    ...\n",
       "1243361    r1  MAADGYLPDWLEDTLSEGISEWWALKPGVPQPKANQQHQDNRRGLV...      1\n",
       "1243362    r1  MAADGYLPDWLEDTLSEGISEWWALKPGVPQPKANQQHQDNRRGLV...      1\n",
       "1243363    r1  MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDGRGLV...      1\n",
       "1243364    r1  MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHQDNSRGLV...      1\n",
       "1243365    r1  MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...      1\n",
       "\n",
       "[1243366 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tsv files\n",
    "df = pd.concat([pd.read_csv(f, sep='\\t').assign(round = os.path.basename(f)[11:13]) for f in files], axis=0)\n",
    "\n",
    "# sum counts within each round\n",
    "df = df.groupby(['round', 'sequence']).sum().reset_index()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unique sequences and total count in each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sequences in each round:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "round\n",
       "r0    801140\n",
       "r1    442226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of unique sequences in each round:\")\n",
    "df.groupby('round').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count for each round:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r0</th>\n",
       "      <td>941365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r1</th>\n",
       "      <td>633141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "round        \n",
       "r0     941365\n",
       "r1     633141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total count for each round:\")\n",
    "total = (df\n",
    "         .drop('sequence', axis=1)\n",
    "         .groupby('round')\n",
    "         .sum()\n",
    "         )\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a higher total count for r0 compared to r1.  We will need to take this into account during modelling by weighting the loss function.\n",
    "\n",
    "Plotting the CDF of counts for the two libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test/eval sets\n",
    "We will need to remove a dataset for evaluation and testing.  Most sequences have low counts, so we try to get some with high counts in each set. \n",
    "\n",
    "For this, we caculate a set of weights according to the model-based enrichment paper, and then include some observations with high and low weights.  The weights are caculated as follows:\n",
    "\n",
    "$$w_i = \\frac{1}{(2\\sigma_i^2)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\sigma_i^2 = \\frac{1}{n_i^{r_1}}(1-\\frac{n_i^{r_1}}{\\sum_i n^{r_1}_i}) + \\frac{1}{n_i^{r_0}}(1-\\frac{n_i^{r_0}}{\\sum_i n^{r_0}_i})$$\n",
    "\n",
    "Where $n_i^{r_0}$ is the count for the $i^{th}$ sequence in the round 0 library, and $n_i^{r_1}$ is the count for the $i^{th}$ sequence in the round 1 library.  This value is high if the counts in both libraries are high, and low if the counts in both libraries are low.  First calculating this for each sequence, and plotting the distribution.\n",
    "\n",
    "We also need to add a pseudo-count of 1 to all observations for this quantitity to be defined.\n",
    "\n",
    "I also calculate the log enrichment (`le`), which we'll compare against the density ratio for the validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>round</th>\n",
       "      <th>sequence</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>le</th>\n",
       "      <th>sig</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.222293</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217095</th>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDGRGLV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.222293</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217096</th>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDSRGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217097</th>\n",
       "      <td>MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHQDNSRGLV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.222293</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217098</th>\n",
       "      <td>MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.222293</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217099</th>\n",
       "      <td>MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777707</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>0.333334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1217100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "round                                             sequence   r0   r1  \\\n",
       "0        MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...  2.0  1.0   \n",
       "1        MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...  2.0  1.0   \n",
       "2        MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...  1.0  2.0   \n",
       "3        MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...  2.0  1.0   \n",
       "4        MAADGYLPDWLEDNLCEGIREWWALKPGAPKPKANQQHQDNARGLV...  2.0  1.0   \n",
       "...                                                    ...  ...  ...   \n",
       "1217095  MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDGRGLV...  1.0  2.0   \n",
       "1217096  MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHKDDSRGLV...  2.0  1.0   \n",
       "1217097  MAADGYLPDWLEDTLSEGISEWWKLKPGPPPPKPAERHQDNSRGLV...  1.0  2.0   \n",
       "1217098  MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...  1.0  2.0   \n",
       "1217099  MAADGYLPDWLEDTLSEGISQWWKLKPGPPPPKPAERHKDDSRGLV...  2.0  1.0   \n",
       "\n",
       "round          le       sig    weight  \n",
       "0       -0.777707  1.499999  0.333334  \n",
       "1       -0.777707  1.499999  0.333334  \n",
       "2        1.222293  1.499999  0.333334  \n",
       "3       -0.777707  1.499999  0.333334  \n",
       "4       -0.777707  1.499999  0.333334  \n",
       "...           ...       ...       ...  \n",
       "1217095  1.222293  1.499999  0.333334  \n",
       "1217096 -0.777707  1.499999  0.333334  \n",
       "1217097  1.222293  1.499999  0.333334  \n",
       "1217098  1.222293  1.499999  0.333334  \n",
       "1217099 -0.777707  1.499999  0.333334  \n",
       "\n",
       "[1217100 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseduo = 1\n",
    "df_unique = (df\n",
    "             # add pseudocunt\n",
    "             .assign(count = lambda x: x['count'] + pseduo)\n",
    "             # r0 and 1 counts in separate columns\n",
    "             .pivot(index='sequence', columns='round', values='count')\n",
    "             .fillna(pseduo)\n",
    "             .reset_index()\n",
    "             # calculate log enrichment and weights as per MBE paper\n",
    "             .assign(\n",
    "                le = lambda x: np.log2((x['r1']/x['r1'].sum())/(x['r0']/x['r0'].sum())),\n",
    "                sig = lambda x: 1/x['r1']*(1-x['r1']/(x['r1'].sum())) + 1/x['r0']*(1-x['r0']/(x['r0'].sum())),\n",
    "                weight = lambda x: 1/(2*x['sig']),\n",
    "                     )\n",
    "             )\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing quantiles for the numeric columns will give us an idea of where might be a good threshold for high and low counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights have a very skewed distribution (as do the counts) - this might be an issue for modelling, since only a few high-count sequences will dominate the results. \n",
    "\n",
    "It would be good to try to keep the test and eval sets roughly balanced, with sequences with high and low weights. For this, we will take equal numbers of sequences from the top 10% and the bottom 90%.  We will need to keep these sets small, since otherwise we'll be removing most of the training data. I'll define 'high-confidence' sequences those weights equal to or higher than 1, and 'low-confidence' sequences those with weights lower than 1.  I'll take 50 sequences from each set for each of the eval and test sets.\n",
    "\n",
    "First, checking how many 'high-confidence' and 'low-confidence' sequences we have with this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set\n",
       "low     1212856\n",
       "high       4244\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 1\n",
    "df_unique = df_unique.assign(set = lambda x: ['high' if w >= thresh else 'low' for w in x['weight']])\n",
    "df_unique.value_counts('set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (1216900, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(set\n",
       " high    50\n",
       " low     50\n",
       " Name: count, dtype: int64,\n",
       " set\n",
       " high    50\n",
       " low     50\n",
       " Name: count, dtype: int64,\n",
       " set\n",
       " low     1212756\n",
       " high       4144\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_eval = df_unique.sample(frac=1).groupby('set').head(100)\n",
    "df_test = df_test_eval.groupby('set').head(50)\n",
    "\n",
    "# remove test set from evaluation set\n",
    "df_eval = df_test_eval[~df_test_eval['sequence'].isin(df_test['sequence'])]\n",
    "df_train = df_unique[~df_unique['sequence'].isin(df_test_eval['sequence'])]\n",
    "print(\"df_train shape: \", df_train.shape)\n",
    "# check we have 50 sequences in each set, and remainder in training set\n",
    "df_eval.value_counts('set'), df_test.value_counts('set'), df_train.value_counts('set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding sequences\n",
    "\n",
    "In this case I one-hot encode the data.  It's more effecient to encode the unique sequences rather than the full training set for model-based enrichment, so I'll do this now and save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodign the sequences with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(len(seq) for seq in df_train['sequence'])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 752, 1280])\n",
      "5\n",
      "737\n",
      "738\n",
      "737\n",
      "738\n",
      "737\n",
      "735\n",
      "737\n"
     ]
    }
   ],
   "source": [
    "def esm_embedding(seqs, max_len):\n",
    "    # print(seqs)\n",
    "    \n",
    "    # data = [(round_val[0], seqs.loc[round_val[0], 'sequence'][:len(seqs.loc[round_val, 'sequence']) - 1]) for round_val in seqs.index.tolist()]\n",
    "    data = [(str(round_val), sequence + \"<pad>\" * (max_len - len(sequence))) for round_val, sequence in seqs]\n",
    "    # print(\"DATAA: \", data[0][1])\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "    # print(\"alphabet: \", alphabet.all_toks)\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter(max_len)\n",
    "    model.eval()\n",
    "\n",
    "    batch_labels, batch_seq, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    # batch_lens is just an array with the length of all of the sequences\n",
    "    # and alphabet.padding_idx is jus thte index of the token '<pad>' in the alpahabet token list\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    print(token_representations.shape)\n",
    "    \n",
    "    # Generate per-sequence representations via averaging\n",
    "    # # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(1))\n",
    "    \n",
    "    print(len(sequence_representations))\n",
    "    print(len(sequence_representations[0]))\n",
    "    print(len(sequence_representations[1]))\n",
    "    \n",
    "    return batch_labels, sequence_representations\n",
    "\n",
    "\n",
    "round_sequence_list = [(round_val, df_train.head().loc[round_val, 'sequence'][:len(df_train.head().loc[round_val, 'sequence']) - 1]) for round_val in df_train.head().index.tolist()]\n",
    "# round_sequence_list[0][1]\n",
    "\n",
    "embeddings = esm_embedding(round_sequence_list, max_seq_len + 10)\n",
    "for x in embeddings[1]:\n",
    "    print(len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a different way to batch the ESM convertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def esm_embedding(seqs, max_len):\n",
    "    # print(seqs)\n",
    "    \n",
    "    # data is a list of tuples (index, sequence)\n",
    "    data = [(str(round_val), sequence + \"<pad>\" * (max_len - len(sequence))) for round_val, sequence in seqs]\n",
    "    \n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter(max_len)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 64\n",
    "    prev = 0\n",
    "    next = batch_size\n",
    "\n",
    "    labels = []\n",
    "    sequence_representations = []\n",
    "    while next <= len(data) and next != prev:\n",
    "\n",
    "        batch_labels, batch_seq, batch_tokens = batch_converter(data[prev:next])\n",
    "        labels = labels + batch_labels\n",
    "        # batch_lens is just an array with the length of all of the sequences\n",
    "        # and alphabet.padding_idx is jus thte index of the token '<pad>' in the alpahabet token list\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        \n",
    "        # Generate per-sequence representations via averaging\n",
    "        # # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "        # sequence_representations = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(1))\n",
    "\n",
    "        prev = next\n",
    "        if next + batch_size > len(data):\n",
    "            next = len(data)\n",
    "        else:\n",
    "            next += batch_size\n",
    "\n",
    "    \n",
    "    return labels, sequence_representations\n",
    "\n",
    "\n",
    "round_sequence_list = [(round_val, df_train.head().loc[round_val, 'sequence'][:len(df_train.head().loc[round_val, 'sequence']) - 1]) for round_val in df_train.head().index.tolist()]\n",
    "# round_sequence_list[0][1]\n",
    "\n",
    "embeddings = esm_embedding(round_sequence_list, max_seq_len + 10)\n",
    "for x in embeddings[1]:\n",
    "    print(len(x))\n",
    "\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_914831/309154787.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out.loc[:,'encoded'] = df_encoded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m df_train_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_train_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "def encode(df, max_len):\n",
    "    # Extract the index (or this case the round number) and the amino acid sequence of each entry in the data frame\n",
    "    # index_and_sequence = [(round_val, df.loc[round_val, 'sequence'][:len(df.loc[round_val, 'sequence']) - 1]) for round_val in df.index.tolist()]\n",
    "    index_and_sequence = [(round_val, df.loc[round_val, 'sequence'].replace('*', '<eos>')) for round_val in df.index.tolist()]\n",
    "    indexes, encoded_tensor = esm_embedding(index_and_sequence, max_len)\n",
    "\n",
    "    numpy_arrays = [tensor.numpy() for tensor in encoded_tensor]\n",
    "    \n",
    "    df_encoded = pd.DataFrame({\n",
    "        'round': indexes,\n",
    "        'encoded': numpy_arrays\n",
    "    })\n",
    "\n",
    "    df_encoded.set_index('round', inplace=True)\n",
    "    df_encoded.index = df_encoded.index.astype('int64')\n",
    "\n",
    "    out = df\n",
    "    # out['encoded'] = df_encoded\n",
    "    out.loc[:,'encoded'] = df_encoded\n",
    "    \n",
    "    return out\n",
    "\n",
    "df_train_test = encode(df_train.head(), 750)\n",
    "\n",
    "# print(df_train_test['encoded'][0])\n",
    "df_train_test['encoded'][0].shape\n",
    "for x in df_train_test['encoded']:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1216900, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recoding\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = df_unique['sequence'].str.len().max() + 10\n",
    "# rerun_encoding = True\n",
    "\n",
    "if not os.path.exists(os.path.join(procdir, 'train_label_encoding.pkl')) or rerun_encoding:\n",
    "    print(\"recoding\")\n",
    "    \n",
    "    df_train = encode(df_train, max_seq_length)\n",
    "    print('after df_train')\n",
    "    df_eval = encode(df_eval, max_seq_length)\n",
    "    print(\"after df_eval\")\n",
    "    df_test = encode(df_test, max_seq_length)\n",
    "    \n",
    "    # save data\n",
    "    df_train.to_pickle(os.path.join(procdir, 'train_ESM_embedding.pkl'))\n",
    "    df_eval.to_pickle(os.path.join(procdir, 'eval_ESM_embedding.pkl'))\n",
    "    df_test.to_pickle(os.path.join(procdir, 'test_ESM_embedding.pkl'))\n",
    "\n",
    "else:\n",
    "    \n",
    "    #df_train = pd.read_pickle(os.path.join(procdir, 'train_ESM_embedding.pkl')) # too large\n",
    "    df_eval = pd.read_pickle(os.path.join(procdir, 'eval_ESM_embedding.pkl'))\n",
    "    df_test = pd.read_pickle(os.path.join(procdir, 'test_ESM_embedding.pkl'))\n",
    "    # max_seq_length would not be defined if the dfs were already loaded in an external file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based enrichment dataset\n",
    "\n",
    "For model based enrichment, we create a dataset where we sample sequences the number of times they were seen in each selection round.  For each sample, the sequence is labelled according to which round it appeared in: -1 for r0 and +1 for r1.  For example, if we have a sequence that was seen once in r0 and twice in r1, it would appear in the final dataset once with a label of -1 and twice with a label of +1.\n",
    "\n",
    "Note that the paper describes using class labels of -1 and +1, and they say they use 'standard logistic loss'.  However, the loss function they describe looks slightly different to the [`BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) in pytorch, probably mostly because the pytorch version expects class labels of 0 and 1 (which are more standard).  Since it's easier (and probably much faster) to the use BCELoss function from pytorch, I'll also use class labels of 0 and 1.\n",
    "\n",
    "The evaluation set would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded</th>\n",
       "      <th>round</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....</td>\n",
       "      <td>r1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               encoded round  count  label\n",
       "0    [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r0    2.0      0\n",
       "0    [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r0    2.0      0\n",
       "1    [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r0    1.0      0\n",
       "2    [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r0    1.0      0\n",
       "3    [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r0    1.0      0\n",
       "..                                                 ...   ...    ...    ...\n",
       "199  [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r1    7.0      1\n",
       "199  [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r1    7.0      1\n",
       "199  [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r1    7.0      1\n",
       "199  [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r1    7.0      1\n",
       "199  [10.0, 0.0, 0.0, 2.0, 5.0, 19.0, 9.0, 12.0, 2....    r1    7.0      1\n",
       "\n",
       "[1058 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add labels to data based on if the sequence base observed in the r0 and r1 libraries\n",
    "df_example = (df_eval\n",
    "                .loc[:, ['encoded', 'r0', 'r1']]\n",
    "                .melt(id_vars = 'encoded', value_vars = ['r0', 'r1'], var_name = 'round', value_name = 'count')\n",
    "                .assign(label = lambda x: (x['round'] == 'r1').astype(int))\n",
    "                )\n",
    "\n",
    "# sample sequences according to their count\n",
    "df_example = df_example.reindex(df_example.index.repeat(df_example['count']))\n",
    "\n",
    "df_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Creating a dataset and dataloader class for these datasets.  I wrap some of the code from earlier in a lightning data module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.,  0.,  0.,  2.,  5., 19.,  9., 12.,  2., 18.,  9.,  3.,  2.,\n",
       "        16.,  9., 15.,  3.,  5.,  7., 14., 13., 18., 18.,  8.,  9.,  8.,\n",
       "        12.,  5., 12., 12., 12., 12.,  8., 12.,  0.,  3., 14.,  6.,  8.,\n",
       "         2.,  2., 15., 14.,  5.,  9., 17.,  9., 12.,  5., 19.,  8., 19.,\n",
       "         9.,  5., 12.,  4., 11.,  5.,  9.,  2.,  8.,  5.,  3., 12., 17.,\n",
       "        11.,  3.,  0.,  2.,  0.,  0.,  0.,  9.,  3.,  6.,  2.,  8.,  0.,\n",
       "        19.,  2., 14., 13.,  9.,  2., 15.,  5.,  2., 11., 12., 19.,  9.,\n",
       "         8., 19., 11.,  6.,  0.,  2.,  0.,  3.,  4., 13.,  3., 14.,  9.,\n",
       "         8.,  3.,  2., 16., 15.,  4.,  5.,  5., 11.,  9.,  5., 14.,  0.,\n",
       "        17.,  4., 13.,  0.,  8.,  8., 14., 17.,  9.,  3., 12.,  9.,  5.,\n",
       "         9., 17.,  3.,  3.,  0.,  0.,  8., 16.,  0., 12.,  5.,  8.,  8.,\n",
       "        14., 12., 17.,  2., 13., 15., 12., 13.,  3., 12.,  2., 15., 15.,\n",
       "        15.,  5., 17.,  5.,  8., 15.,  5.,  8., 13., 12.,  0., 14.,  8.,\n",
       "        14.,  9., 11.,  4.,  5., 13., 16.,  5.,  2., 15.,  3., 15., 17.,\n",
       "        12.,  2., 12., 13., 12.,  9.,  5.,  3., 12., 12.,  0.,  0., 12.,\n",
       "        16., 15.,  9.,  5., 15., 11., 16., 10.,  0., 15.,  5.,  5.,  5.,\n",
       "         0., 12., 10.,  0.,  2., 11., 11.,  3.,  5.,  0.,  2.,  5., 17.,\n",
       "         5., 15., 15., 15.,  5., 11., 18.,  6.,  1.,  2., 15., 13., 18.,\n",
       "         9.,  5.,  2., 14., 17.,  7., 16., 16., 15., 16., 14., 16., 18.,\n",
       "         0.,  9., 12., 16., 19., 11., 11.,  6.,  9., 19.,  8., 13.,  7.,\n",
       "        15., 11.,  5., 16., 15.,  5.,  5.,  0., 16., 11.,  2., 11., 16.,\n",
       "        19.,  4.,  5., 19., 15., 16., 12., 18.,  5., 19.,  4.,  2.,  4.,\n",
       "        11., 14.,  4.,  6.,  1.,  6.,  4., 15., 12., 14.,  2., 18., 13.,\n",
       "        14.,  9.,  7., 11., 11., 11., 18.,  5.,  4., 14., 12.,  8., 14.,\n",
       "         9., 11.,  4.,  8.,  9.,  4., 11.,  7., 13., 17.,  8.,  3., 17.,\n",
       "        16.,  2., 11., 11.,  5., 17.,  8., 16.,  7.,  0., 11., 11.,  9.,\n",
       "        16., 15., 16., 17., 13., 17.,  4., 16.,  2., 15.,  2., 19., 13.,\n",
       "         9., 12., 19., 17.,  9.,  5., 15.,  0.,  6., 13.,  5.,  1.,  9.,\n",
       "        12., 12.,  4., 12.,  0.,  2., 17.,  4., 10., 17., 12., 13., 19.,\n",
       "         5., 19.,  9., 16.,  9., 11., 11.,  5., 15., 13.,  0., 17.,  5.,\n",
       "        14., 15., 15.,  4., 19.,  1.,  9.,  3., 19.,  4., 12., 15., 13.,\n",
       "        10.,  9., 14., 16.,  5., 11., 11.,  4., 13.,  4., 15., 19., 16.,\n",
       "         4.,  3.,  2., 17., 12.,  4.,  6., 15., 15., 19.,  0.,  6., 15.,\n",
       "        13., 15.,  9.,  2., 14.,  9., 10., 11., 12.,  9.,  7.,  2., 13.,\n",
       "        19.,  9., 19., 19.,  9., 11., 14., 16., 13.,  5., 16., 16., 15.,\n",
       "         5., 16., 16., 11., 13., 15., 14.,  9.,  9.,  4., 15., 13.,  0.,\n",
       "         5., 12., 13., 15., 10., 15.,  9., 13.,  0., 14., 11., 18.,  9.,\n",
       "        12.,  5., 12.,  1., 19., 14., 13., 13., 14.,  9., 15.,  8., 16.,\n",
       "         0., 11.,  2., 11., 11., 11., 15., 11.,  4., 12., 18., 16.,  0.,\n",
       "         0., 15.,  8., 19.,  6.,  9., 11.,  5., 14.,  2., 15.,  9., 17.,\n",
       "        11., 12.,  5., 12.,  0., 10.,  0., 15.,  6.,  8.,  2.,  2.,  3.,\n",
       "         3.,  8.,  4.,  4., 12., 10.,  6.,  5., 11.,  9.,  7.,  4.,  5.,\n",
       "         8.,  3.,  5., 16., 16.,  0., 15., 11.,  0.,  3.,  9.,  2., 11.,\n",
       "        17., 10.,  7., 16.,  2.,  3.,  3.,  3.,  7., 14., 16., 16., 11.,\n",
       "        12., 17.,  0., 16.,  3., 13., 19.,  5., 16., 17.,  0., 11., 11.,\n",
       "         9., 13., 15., 15., 11., 16.,  0., 12., 16., 16., 14., 16., 17.,\n",
       "        11.,  2., 13.,  5.,  0.,  9., 12.,  5., 10., 17., 18., 13.,  2.,\n",
       "        14.,  2., 17., 19.,  9., 13.,  5., 12.,  7., 18.,  0.,  8.,  7.,\n",
       "        12.,  6., 16.,  2.,  5.,  6.,  4.,  6., 12., 15., 12.,  9., 10.,\n",
       "         5.,  5.,  4.,  5.,  9.,  8.,  6., 12., 12., 12., 13.,  7., 10.,\n",
       "         7.,  8., 11., 16., 12., 17., 12.,  0., 11., 12., 12., 16., 16.,\n",
       "         4., 15., 12.,  0.,  8.,  4.,  0., 15.,  4.,  7., 16., 13., 19.,\n",
       "        15., 16.,  5., 13., 17., 15., 17.,  3.,  7.,  3., 18.,  3.,  9.,\n",
       "        13.,  8.,  3., 11., 15.,  8., 14., 18., 11., 12.,  3.,  7., 13.,\n",
       "        19., 16., 15., 11., 19., 11.,  8., 15., 17., 11., 17.,  2.,  4.,\n",
       "        16., 17.,  2., 16., 11.,  5., 17., 19., 15.,  3., 12., 14., 12.,\n",
       "         7.,  5., 16., 14., 19.,  9., 16., 14., 11.,  9., 20.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MBEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, file_name):\n",
    "        self.df_raw = df\n",
    "        self.df = self.create_mbe_dataset(df)\n",
    "        self.ddf = dd.read_parquet(os.path.join(procdir, file_name))\n",
    "    \n",
    "    def create_mbe_dataset(self, df):\n",
    "\n",
    "        df = (df\n",
    "              .loc[:, ['encoded', 'r0', 'r1']]\n",
    "              .melt(id_vars=['encoded'], value_vars=['r0', 'r1'], var_name='round', value_name='value')\n",
    "              .assign(label = lambda x: (x['round'] == 'r1').astype(int))\n",
    "        )\n",
    "        df = df.reindex(df.index.repeat(df['value'])).reset_index()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Number of examples in each class\n",
    "        \"\"\"\n",
    "        counts = (self.df\n",
    "                .groupby('label')\n",
    "                .size()\n",
    "               )\n",
    "        return torch.tensor(counts.loc[1] / counts.loc[0], dtype=torch.float64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: instead of getting it from the data set, get it from the file...\n",
    "        # row = self.df.iloc[idx]\n",
    "        row = self.ddf.loc[idx].compute()\n",
    "        return row['encoded'], float(row['label'])\n",
    "\n",
    "class LEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Log enrichment dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: instead of getting it from the data set, get it from the file...\n",
    "        row = self.df.iloc[idx]\n",
    "        return row['encoded'], row['le']\n",
    "\n",
    "class MBEDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, enc=\"ESM_embedding\", stage=None):\n",
    "        self.train = pd.read_pickle(os.path.join(self.data_dir, f'train_{enc}.pkl'))\n",
    "        self.eval = pd.read_pickle(os.path.join(self.data_dir, f'eval_{enc}.pkl'))\n",
    "        self.test = pd.read_pickle(os.path.join(self.data_dir, f'test_{enc}.pkl'))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MBEDataset(self.train, 'train_ESM_embedding.pkl'), batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(LEDataset(self.eval), batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(LEDataset(self.test), batch_size=self.batch_size)\n",
    "    \n",
    "# test out dataset class\n",
    "ds = MBEDataset(df_eval, 'eval_label_encoding.pkl')\n",
    "ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0544, dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.,  0.,  0.,  2.,  5., 19.,  9., 12.,  2., 18.,  9.,  3.,  2.,\n",
       "        16.,  9., 15.,  3.,  5.,  7., 14., 13., 18., 18.,  8.,  9.,  8.,\n",
       "        12.,  5., 12., 12., 12., 12.,  8., 12.,  0.,  3., 14.,  6.,  8.,\n",
       "         2.,  2., 15., 14.,  5.,  9., 17.,  9., 12.,  5., 19.,  8., 19.,\n",
       "         9.,  5., 12.,  4., 11.,  5.,  9.,  2.,  8.,  5.,  3., 12., 17.,\n",
       "        11.,  3.,  0.,  2.,  0.,  0.,  0.,  9.,  3.,  6.,  2.,  8.,  0.,\n",
       "        19.,  2., 14., 13.,  9.,  2., 15.,  5.,  2., 11., 12., 19.,  9.,\n",
       "         8., 19., 11.,  6.,  0.,  2.,  0.,  3.,  4., 13.,  3., 14.,  9.,\n",
       "         8.,  3.,  2., 16., 15.,  4.,  5.,  5., 11.,  9.,  5., 14.,  0.,\n",
       "        17.,  4., 13.,  0.,  8.,  8., 14., 17.,  9.,  3., 12.,  9.,  5.,\n",
       "         9., 17.,  3.,  3.,  0.,  0.,  8., 16.,  0., 12.,  5.,  8.,  8.,\n",
       "        14., 12., 17.,  2., 13., 15., 12., 13.,  3., 12.,  2., 15., 15.,\n",
       "        15.,  5., 17.,  5.,  8., 15.,  5.,  8., 13., 12.,  0., 14.,  8.,\n",
       "        14.,  9., 11.,  4.,  5., 13., 16.,  5.,  2., 15.,  3., 15., 17.,\n",
       "        12.,  2., 12., 13., 12.,  9.,  5.,  3., 12., 12.,  0.,  0., 12.,\n",
       "        16., 15.,  9.,  5., 15., 11., 16., 10.,  0., 15.,  5.,  5.,  5.,\n",
       "         0., 12., 10.,  0.,  2., 11., 11.,  3.,  5.,  0.,  2.,  5., 17.,\n",
       "         5., 15., 15., 15.,  5., 11., 18.,  6.,  1.,  2., 15., 13., 18.,\n",
       "         9.,  5.,  2., 14., 17.,  7., 16., 16., 15., 16., 14., 16., 18.,\n",
       "         0.,  9., 12., 16., 19., 11., 11.,  6.,  9., 19.,  8., 13.,  7.,\n",
       "        15., 11.,  5., 16., 15.,  5.,  5.,  0., 16., 11.,  2., 11., 16.,\n",
       "        19.,  4.,  5., 19., 15., 16., 12., 18.,  5., 19.,  4.,  2.,  4.,\n",
       "        11., 14.,  4.,  6.,  1.,  6.,  4., 15., 12., 14.,  2., 18., 13.,\n",
       "        14.,  9.,  7., 11., 11., 11., 18.,  5.,  4., 14., 12.,  8., 14.,\n",
       "         9., 11.,  4.,  8.,  9.,  4., 11.,  7., 13., 17.,  8.,  3., 17.,\n",
       "        16.,  2., 11., 11.,  5., 17.,  8., 16.,  7.,  0., 11., 11.,  9.,\n",
       "        16., 15., 16., 17., 13., 17.,  4., 16.,  2., 15.,  2., 19., 13.,\n",
       "         9., 12., 19., 17.,  9.,  5., 15.,  0.,  6., 13.,  5.,  1.,  9.,\n",
       "        12., 12.,  4., 12.,  0.,  2., 17.,  4., 10., 17., 12., 13., 19.,\n",
       "         5., 19.,  9., 16.,  9., 11., 11.,  5., 15., 13.,  0., 17.,  5.,\n",
       "        14., 15., 15.,  4., 19.,  1.,  9.,  3., 19.,  4., 12., 15., 13.,\n",
       "        10.,  9., 14., 16.,  5., 11., 11.,  4., 13.,  4., 15., 19., 16.,\n",
       "         4.,  3.,  2., 17., 12.,  4.,  6., 15., 15., 19.,  0.,  6., 15.,\n",
       "        13., 15.,  9.,  2., 14.,  9., 10., 11., 12.,  9.,  7.,  2., 13.,\n",
       "        19.,  9., 19., 19.,  9., 11., 14., 16., 13.,  5., 16., 16., 15.,\n",
       "         5., 16., 16., 11., 13., 15., 14.,  9.,  9.,  4., 15., 13.,  0.,\n",
       "         5., 12., 13., 15., 10., 15.,  9., 13.,  0., 14., 11., 18.,  9.,\n",
       "        12.,  5., 12.,  1., 19., 14., 13., 13., 14.,  9., 15.,  8., 16.,\n",
       "         0., 11.,  2., 11., 11., 11., 15., 11.,  4., 12., 18., 16.,  0.,\n",
       "         0., 15.,  8., 19.,  6.,  9., 11.,  5., 14.,  2., 15.,  9., 17.,\n",
       "        11., 12.,  5., 12.,  0., 10.,  0., 15.,  6.,  8.,  2.,  2.,  3.,\n",
       "         3.,  8.,  4.,  4., 12., 10.,  6.,  5., 11.,  9.,  7.,  4.,  5.,\n",
       "         8.,  3.,  5., 16., 16.,  0., 15., 11.,  0.,  3.,  9.,  2., 11.,\n",
       "        17., 10.,  7., 16.,  2.,  3.,  3.,  3.,  7., 14., 16., 16., 11.,\n",
       "        12., 17.,  0., 16.,  3., 13., 19.,  5., 16., 17.,  0., 11., 11.,\n",
       "         9., 13., 15., 15., 11., 16.,  0., 12., 16., 16., 14., 16., 17.,\n",
       "        11.,  2., 13.,  5.,  0.,  9., 12.,  5., 10., 17., 18., 13.,  2.,\n",
       "        14.,  2., 17., 19.,  9., 13.,  5., 12.,  7., 18.,  0.,  8.,  7.,\n",
       "        12.,  6., 16.,  2.,  5.,  6.,  4.,  6., 12., 15., 12.,  9., 10.,\n",
       "         5.,  5.,  4.,  5.,  9.,  8.,  6., 12., 12., 12., 13.,  7., 10.,\n",
       "         7.,  8., 11., 16., 12., 17., 12.,  0., 11., 12., 12., 16., 16.,\n",
       "         4., 15., 12.,  0.,  8.,  4.,  0., 15.,  4.,  7., 16., 13., 19.,\n",
       "        15., 16.,  5., 13., 17., 15., 17.,  3.,  7.,  3., 18.,  3.,  9.,\n",
       "        13.,  8.,  3., 11., 15.,  8., 14., 18., 11., 12.,  3.,  7., 13.,\n",
       "        19., 16., 15., 11., 19., 11.,  8., 15., 17., 11., 17.,  2.,  4.,\n",
       "        16., 17.,  2., 16., 11.,  5., 17., 19., 15.,  3., 12., 14., 12.,\n",
       "         7.,  5., 16., 14., 19.,  9., 16., 14., 11.,  9., 20.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " -0.7777074994126139)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LEDataset(df_eval)\n",
    "le[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " tensor([1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 0., 1.], dtype=torch.float64)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out datamodule class - takes a while to load pickled data\n",
    "dm = MBEDataModule(procdir, batch_size=BATCH_SIZE)\n",
    "dm.setup()\n",
    "test = next(iter(dm.train_dataloader()))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this case, we'll use a simple feedforward network with two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0265],\n",
       "        [ 0.0588],\n",
       "        [ 0.0310],\n",
       "        [ 0.0127],\n",
       "        [-0.0194]], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MBELogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for logistic regression. \n",
    "    Returns logits by default, or probabilities if probs=True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, 1, dtype = torch.float64)\n",
    "        self.pos_weight = 1 if pos_weight is None else pos_weight\n",
    "\n",
    "    def forward(self, x, probs=False):\n",
    "        \"\"\"\n",
    "        Return logits or probabilities\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        logits = self.linear(x)\n",
    "        if probs:\n",
    "            return torch.sigmoid(logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return density ratio, an estimate of log enrichment\n",
    "        \"\"\"\n",
    "        \n",
    "        # get probability for positive class\n",
    "        p = self.forward(x, probs=True)\n",
    "\n",
    "        # density ratio is p/(1-p), adjusted for class imbalance\n",
    "        return p/(1-p) / self.pos_weight\n",
    "\n",
    "class MBEFeedForward(MBELogisticRegression):\n",
    "\n",
    "    def __init__(self, input_size, pos_weight=None, n_units=128):\n",
    "        super().__init__(input_size, pos_weight)\n",
    "        self.n_units = n_units\n",
    "        self.pos_weight = pos_weight\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_size, n_units, dtype = torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, 1, dtype = torch.float64),\n",
    "        )\n",
    "\n",
    "\n",
    "model = MBEFeedForward(len(ds[0][0]), pos_weight=ds.get_weights())\n",
    "model(test[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9739],\n",
       "        [1.0059],\n",
       "        [0.9783],\n",
       "        [0.9605],\n",
       "        [0.9302]], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test[0])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a weighted BCE loss function with the Adam optimizer.  The classes are weighted using the ratio of total reads in r1 vs r0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMBE(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr=0.001, pos_weight = 1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.spearman = stats.spearmanr \n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x).squeeze()\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss, on_step = True, on_epoch = True, prog_bar = True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            le = self.model.predict(x).squeeze().cpu().numpy()\n",
    "            spearman = self.spearman(le, y.cpu().numpy()).statistic\n",
    "            self.log('val_spearman', spearman, on_step = False, on_epoch = True, prog_bar = True)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/tlorant@cmri.com.au/aavolve_data/.micromamba/envs/default/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240618_233743-hvzhjdbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lorant/MBE/runs/hvzhjdbi' target=\"_blank\">feedforward</a></strong> to <a href='https://wandb.ai/lorant/MBE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lorant/MBE' target=\"_blank\">https://wandb.ai/lorant/MBE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lorant/MBE/runs/hvzhjdbi' target=\"_blank\">https://wandb.ai/lorant/MBE/runs/hvzhjdbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A10') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/home/tlorant@cmri.com.au/aavolve_data/.micromamba/envs/default/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c17634c5d75443fada2af01ee520c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_spearman         -0.008669466711580753\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | MBEFeedForward    | 2.0 M \n",
      "1 | loss  | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d906a3496ba40609a7c5a14c7c4e939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/tlorant@cmri.com.au/aavolve_data/.micromamba/envs/default/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n",
      "/data/home/tlorant@cmri.com.au/aavolve_data/.micromamba/envs/default/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fdc2e81c544b98998b5941d56864b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6c01455e7c42a5bae3ffe1a6ccf6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd2ebfaa4345f7bf46fc5c9c662073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cf280fcde546ecb596bb6cf66c68d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed6f0b574354754b72207f9af2adc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b1767515ae471ab6382259fac7f59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d4f1b610254859a4f5d93628fd0d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04b02ac7cfe4d5b89a7de47817c76f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fad1b8593494a519efc0d45fd198827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d85f46f08564dc5905cde2d765afdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d856adf6315940058d455ebdfca150a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0439dd7a3c594eb0bec2767fbce8a02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▁▃▃▃▂▁▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇▅▃▃▃▄▄█▄▅▄▅▃▄▅▃▅▅▃▄▆▄▄▆▃▄▄▆▇▅▃▄▄▄▄▂▅▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_spearman</td><td>▁█▇█▆▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss_epoch</td><td>0.70579</td></tr><tr><td>train_loss_step</td><td>0.72748</td></tr><tr><td>trainer/global_step</td><td>626039</td></tr><tr><td>val_spearman</td><td>0.36864</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feedforward</strong> at: <a href='https://wandb.ai/lorant/MBE/runs/hvzhjdbi' target=\"_blank\">https://wandb.ai/lorant/MBE/runs/hvzhjdbi</a><br/> View project at: <a href='https://wandb.ai/lorant/MBE' target=\"_blank\">https://wandb.ai/lorant/MBE</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240618_233743-hvzhjdbi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate lightning model\n",
    "lit_model = LitMBE(model, pos_weight=ds.get_weights())\n",
    "\n",
    "# use weights and biases logger\n",
    "wandb_logger = WandbLogger(project='mbe', name = \"feedforward\")\n",
    "wandb_logger.experiment.config.update({\n",
    "    \"lr\": 0.001,\n",
    "    \"pos_weight\": ds.get_weights(),\n",
    "    \"n_units\": 128,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"arch\": \"feedforward\",\n",
    "    \"enc\": \"onehot\",\n",
    "    \"loss\": \"BCEWithLogitsLoss\",\n",
    "    \"opt\": \"Adam\"\n",
    "})\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(max_epochs=10, logger = wandb_logger)\n",
    "trainer.validate(model = lit_model, datamodule = dm)\n",
    "trainer.fit(model = lit_model, datamodule = dm)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future improvements / things to try:\n",
    "\n",
    " - Weight decay / regularization\n",
    " - Different architechtures\n",
    " - Different encoding (label encoding / embeddings from ESM-2)\n",
    " - Weight decay / regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
